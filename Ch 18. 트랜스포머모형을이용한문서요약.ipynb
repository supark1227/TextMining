{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb13b8a",
   "metadata": {},
   "source": [
    "# Chapter 18. 트랜스포머 모형을 이용한 문서 요약\n",
    "## 1. 문서 요약의 이해\n",
    "## 2. 파이프라인을 이용한 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb60ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약문:\n",
      " [{'summary_text': ' Text mining involves deriving high-quality information from text . Written resources may include websites, books, emails, reviews, and articles . Text mining is similar to text analytics . It involves the discovery by computer of new, previously unknown information by automatically extracting information from different written resources .'}]\n",
      "원문 길이: 778 요약문 길이: 341\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 문서요약을 위한 파이프라인 생성\n",
    "summarizer = pipeline(\"summarization\")\n",
    "# 요약 대상 원문 - 텍스트마이닝의 정의(Wikipedia)\n",
    "text = '''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.''' \n",
    "summary_text = summarizer(text) #파이프라인으로 문서요약 수행\n",
    "print(\"요약문:\\n\", summary_text)\n",
    "print(\"원문 길이:\", len(text), \"요약문 길이:\", len(summary_text[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe795a",
   "metadata": {},
   "source": [
    "## 3. T5 모형과 자동 클래스를 이용한 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75d067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer type: <class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "model type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "print(\"tokenizer type:\", type(tokenizer))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "print(\"model type:\", type(model))\n",
    "# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c71c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문에 필요한 전처리를 수행. 여기서는 strip()을 적용하고 \\n(줄바꿈)을 제거\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "# 전처리 결과 앞에 summarize: 를 추가 - 모형의 task를 summarize(문서요약)로 지정\n",
    "input_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "# 입력 원문을 토크나이즈\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205d21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      " text data mining is the process of deriving high-quality information from text. it involves the discovery by computer of new, previously unknown information. a KDD (Knowledge Discovery in Databases) process is similar to text analytics.\n",
      "Original text length: 778 Summarized text length: 236\n"
     ]
    }
   ],
   "source": [
    "# 요약문생성\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, # beam의 길이\n",
    "                             no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
    "                             min_length=30,  #요약문의 최소 토큰 수\n",
    "                             max_length=100,  #요약문의 최대 토큰 수\n",
    "                             early_stopping=True) #EOS 토큰을 만나면 종료\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"Summarized text: \\n\",output)\n",
    "print(\"Original text length:\", len(text), \"Summarized text length:\", len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e9e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated text: \n",
      " Das ist gut.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate english to german: That is good\"\n",
    "\n",
    "# 입력 원문을 토크나이즈\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "result = model.generate(tokenized_text, \n",
    "                         num_beams=4, # beam의 길이\n",
    "                         no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
    "                         max_length=100,  #요약문의 최대 토큰 수\n",
    "                         early_stopping=True) #EOS 토큰을 만나면 종료max_new_tokens=100, do_sample=False)\n",
    "output = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "print (\"translated text: \\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3aeeec",
   "metadata": {},
   "source": [
    "## 4. T5 모형과 트레이너를 이용한 미세조정학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b83a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small', model_max_length=1024)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fccd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      " the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in history. no one making under $400,000 per year will pay a penny more in taxes.\n",
      "Original text length: 441 Summarized text length: 241\n"
     ]
    }
   ],
   "source": [
    "text = '''The Inflation Reduction Act lowers prescription drug costs, health care costs, \n",
    "and energy costs. It's the most aggressive action on tackling the climate crisis in American history, \n",
    "which will lift up American workers and create good-paying, union jobs across the country. \n",
    "It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. \n",
    "And no one making under $400,000 per year will pay a penny more in taxes.'''\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "# 전처리 결과 앞에 summarize: 를 추가 - 모형의 task를 summarize(문서요약)로 지정\n",
    "input_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "# 입력 원문을 토크나이즈\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "summary_ids = model.generate(tokenized_text, \n",
    "                         num_beams=4, # beam의 길이\n",
    "                         no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
    "                         min_length=30,  #요약문의 최소 토큰 수\n",
    "                         max_length=100,  #요약문의 최대 토큰 수\n",
    "                         early_stopping=True) #EOS 토큰을 만나면 종료)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print (\"Summarized text: \\n\",output)\n",
    "print(\"Original text length:\", len(text), \"Summarized text length:\", len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87eb8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (C:/Users/user/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BillSum 데이터 예 - 첫 항목\n",
      "\tText: The people of the State of California do enact as \n",
      "\tSummary: The California Prompt Payment Act dictates that a \n",
      "\tTitle: An act to amend Section\n",
      "927\n",
      "927.2\n",
      "of the Governmen\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "example = billsum[\"train\"][0]\n",
    "print(\"BillSum 데이터 예 - 첫 항목\")\n",
    "print(\"\\tText:\", example['text'][:50])\n",
    "print(\"\\tSummary:\", example['summary'][:50])\n",
    "print(\"\\tTitle:\", example['title'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d28b1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70209c7ec8a140e8b50e27bbeaadf60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822f426a760f4d0598a82d5006311df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 989\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(data):\n",
    "    # 법안 원본 앞에 \"summarize: \"를 붙임\n",
    "    inputs = [\"summarize: \" + doc for doc in data[\"text\"]]\n",
    "    # 입력 텍스트를 토크나이즈\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    # 라벨로 사용할 요약문을 토크나이즈\n",
    "    labels = tokenizer(data[\"summary\"], max_length=128, truncation=True)\n",
    "    # model_inputs의 labels 항목으로 요약문 토크나이즈 결과를 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 전처리 함수를 데이터에 적용, 원래 billsum에 있던 항목들은 제거\n",
    "tokenized_billsum = billsum.map(preprocess_text, batched=True, remove_columns=billsum[\"train\"].column_names)\n",
    "tokenized_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9fbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd0dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab0f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 생성한 요약 토큰을 텍스트로 디코드\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 라벨에서 디코드할 수 없는 -100을 교체\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # 라벨을 텍스트로 디코드\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # 디코드된 요약문과 라벨로 ROUGE 스코어 계산\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2657b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 989\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248\n",
      "  Number of trainable parameters = 60506624\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 01:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.808137</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.597031</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.534104</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.516838</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=248, training_loss=3.0233981224798385, metrics={'train_runtime': 72.1547, 'train_samples_per_second': 54.827, 'train_steps_per_second': 3.437, 'total_flos': 1070824333246464.0, 'train_loss': 3.0233981224798385, 'epoch': 4.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",         # 모형 예측과 체크포인트 저장 폴더, 반드시 필요\n",
    "    evaluation_strategy=\"epoch\",    # 평가 단위, 여기서는 epoch를 선택\n",
    "    learning_rate=2e-5,             # 학습률\n",
    "    per_device_train_batch_size=16, # 학습에 사용할 배치 크기\n",
    "    per_device_eval_batch_size=16,  # 평가에 사용할 배치 크기\n",
    "    weight_decay=0.01,              # 가중치 감쇠 값\n",
    "    save_total_limit=3,             # 저장할 체크포인트의 최대값\n",
    "    num_train_epochs=4,             # 에포크 수\n",
    "    predict_with_generate=True,     # 평가지표(ROUGE) 계산을 위해 generate할 지의 여부\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e878d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      " the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. No one making under $400,000 per year will pay a penny more in taxes.\n",
      "Original text length: 441 Summarized text length: 341\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(tokenized_text, \n",
    "                         num_beams=4, # beam의 길이\n",
    "                         no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
    "                         min_length=30,  #요약문의 최소 토큰 수\n",
    "                         max_length=100,  #요약문의 최대 토큰 수\n",
    "                         early_stopping=True) #EOS 토큰을 만나면 종료max_new_tokens=100, do_sample=False)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print (\"Summarized text: \\n\",output)\n",
    "print(\"Original text length:\", len(text), \"Summarized text length:\", len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a18415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to summary\n",
      "Configuration saved in summary\\config.json\n",
      "Model weights saved in summary\\pytorch_model.bin\n",
      "tokenizer config file saved in summary\\tokenizer_config.json\n",
      "Special tokens file saved in summary\\special_tokens_map.json\n",
      "Copy vocab file to summary\\spiece.model\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./summary\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./summary\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./summary.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"summary\")  # 모형 저장\n",
    "# 저장된 모형 로드\n",
    "tokenizer = T5TokenizerFast.from_pretrained('./summary')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee27db",
   "metadata": {},
   "source": [
    "## 5. 한글 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fdab7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"디아블로는 액션 롤플레잉 핵 앤드 슬래시 비디오 게임이다. \n",
    "플레이어는 주변 환경을 마우스로 사용해 영웅을 움직이게 한다. \n",
    "주문을 외는 등의 다른 활동은 키보드 입력으로 이루어진다. \n",
    "플레이어는 이 게임에서 장비를 획득하고, 주문을 배우고, 적을 쓰러뜨리며, NPC와 대화를 나눌 수 있다.\n",
    "지하 미궁은 주어진 형식이 있고 부분적으로 반복되는 형태가 존재하나 전체적으로 보면 무작위로 생성된다. \n",
    "예를 들어 지하 묘지의 경우에는 긴 복도와 닫힌 문들이 존재하고, 동굴은 좀 더 선형 형태를 띠고 있다. \n",
    "플레이어에게는 몇몇 단계에서 무작위의 퀘스트를 받는다. \n",
    "이 퀘스트는 선택적인 사항이나 플레이어의 영웅들을 성장시키거나 줄거리를 이해하는데 도움을 준다. \n",
    "그러나 맨 뒤에 두 퀘스트는 게임을 끝내기 위해 완료시켜야 한다.\"\"\"\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddceacd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-summarization\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--gogamza--kobart-summarization\\snapshots\\8a63d6913edc0e16a902e3fa8b688a134f0dd776\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-summarization.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디아블로는 액션 롤플레잉 핵 앤드 슬래시 비디오 게임이다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, # beam의 길이\n",
    "                             no_repeat_ngram_size=3, #동어 반복을 피하기 위해 사용\n",
    "                             min_length=10,  #요약문의 최소 토큰 수\n",
    "                             max_length=150,  #요약문의 최대 토큰 수\n",
    "                             early_stopping=True) #EOS 토큰을 만나면 종료\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2e0fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 0.6,\n",
      "  \"max_length\": 84,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 0.6,\n",
      "  \"max_length\": 84,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 0.6,\n",
      "  \"max_length\": 84,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"length_penalty\": 0.6,\n",
      "  \"max_length\": 84,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"no_repeat_ngram_size\": 2,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c54da57f99647b9904b4067746bb2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum\\snapshots\\2437a524effdbadc327ced84595508f1e32025b3\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at csebuetnlp/mT5_multilingual_XLSum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디아블 게임의 이야기를 들어봤다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, # beam의 길이\n",
    "                             no_repeat_ngram_size=2, #동어 반복을 피하기 위해 사용\n",
    "                             min_length=10,  #요약문의 최소 토큰 수\n",
    "                             max_length=150,  #요약문의 최대 토큰 수\n",
    "                             early_stopping=True) #EOS 토큰을 만나면 종료\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9bbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "de5c66b24b88d6fd3b2df5c0958b0d2eda73064658479df36c55d94ea0f77ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
