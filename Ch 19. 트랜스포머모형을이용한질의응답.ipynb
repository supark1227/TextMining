{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1dee777",
   "metadata": {},
   "source": [
    " # Chapter 19. 트랜스포머 모형을 이용한 질의 응답\n",
    "## 1. 질의 응답 시스템의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b18f76",
   "metadata": {},
   "source": [
    "## 2. 파이프라인을 이용한 질의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda8ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4241906404495239, 'start': 103, 'end': 161, 'answer': 'the process of deriving high-quality information from text'}\n",
      "질의: What are the perspectives of text mining?\n",
      "응답: information extraction, data mining, and a KDD\n",
      "응답에 사용된 context: information extraction, data mining, and a KDD\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "context = r'''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.''' \n",
    "question = \"What is text mining?\"\n",
    "answer = question_answerer(question=question, context=context)\n",
    "print(answer)\n",
    "question2 = \"What are the perspectives of text mining?\"\n",
    "answer2 = question_answerer(question=question2, context=context)\n",
    "print(\"질의:\", question2)\n",
    "print(\"응답:\", answer2['answer'])\n",
    "print(\"응답에 사용된 context:\", context[answer2['start']:answer2['end']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0b8d9",
   "metadata": {},
   "source": [
    "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    "2. Define a text and a few questions.\n",
    "3. Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators, token type ids and attention masks.\n",
    "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "5. Compute the softmax of the result to get probabilities over the tokens.\n",
    "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    "7. Print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac735e",
   "metadata": {},
   "source": [
    "## 3. 자동 클래스를 이용한 질의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49b1c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer type: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "model type: <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering'>\n",
      "output type <class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "print(\"tokenizer type:\", type(tokenizer))\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "print(\"model type:\", type(model))\n",
    "# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 질문과 context를 함께 토큰화\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"output type\", type(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f121eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: tensor(35, device='cuda:0') , end: tensor(46, device='cuda:0')\n",
      "질의: What is text mining?\n",
      "응답: the process of deriving high - quality information from text\n"
     ]
    }
   ],
   "source": [
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \", end:\", answer_end)\n",
    "\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0] \n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환하고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    ")\n",
    "\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45168be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7526955008506775, 'start': 31, 'end': 34, 'answer': 'Bob'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "text = \"Alice is sitting on the bench. Bob is sitting next to her.\"\n",
    "\n",
    "result = question_answerer(question=\"Who is the CEO?\", context=text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9938e2f",
   "metadata": {},
   "source": [
    "## 4. 트레이너를 이용한 질의 응답 미세조정학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc3ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "# distilbert-base-uncased는 질의응답을 위해 사전학습된 모델이 아니기 때문에 질의응답이 불가\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ca788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: tensor(67, device='cuda:0') , end: tensor(12, device='cuda:0')\n",
      "질의: The dance capital of the world is what city in the US?\n",
      "응답: \n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"The city is the birthplace of many cultural movements, including the Harlem \n",
    "        Renaissance in literature and visual art; abstract expressionism \n",
    "        (also known as the New York School) in painting; and hip hop, punk, salsa, disco, \n",
    "        freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered \n",
    "        the dance capital of the world. The city is also widely celebrated in popular lore, \n",
    "        frequently the setting for books, movies (see List of films set in New York City), \n",
    "        and television programs.\"\"\"\n",
    "question = \"The dance capital of the world is what city in the US?\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \", end:\", answer_end)\n",
    "\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0] \n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환하고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    ")\n",
    "\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer) #응답을 만들지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e92ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '56ce1225aab44d1400b88432',\n",
       " 'title': 'Frédéric_Chopin',\n",
       " 'context': 'Fryderyk Chopin was born in Żelazowa Wola, 46 kilometres (29 miles) west of Warsaw, in what was then the Duchy of Warsaw, a Polish state established by Napoleon. The parish baptismal record gives his birthday as 22 February 1810, and cites his given names in the Latin form Fridericus Franciscus (in Polish, he was Fryderyk Franciszek). However, the composer and his family used the birthdate 1 March,[n 2] which is now generally accepted as the correct date.',\n",
       " 'question': 'When was his birthday recorded as being?',\n",
       " 'answers': {'text': ['22 February 1810'], 'answer_start': [212]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce5f44aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e69a0cf21474530843286051c5909fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292a62fa71f04a91b67a56d2135ad05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    questions = [q.strip() for q in data[\"question\"]] # 질의 추출하고 전처리\n",
    "    # 질의와 context를 함께 토큰화\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        data[\"context\"],\n",
    "        max_length=384,              # 토큰화 결과의 최대 길이\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True, # offset_mapping을 반환\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = data[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0] # context에서 응답 시작 위치\n",
    "        # context에서 응답 종료 위치를 계산\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # sequence_ids를 이용해 context 토큰의 시작과 끝을 알아냄\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: # sequence_ids에서 첫 1의 위치\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1: # sequence_ids에서 마지막 1의 위치\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # 응답이 context 안에 있지 않으면 응답의 시작위치와 종료위치를 (0, 0)으로 set\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # 응답이 context 안에 있으면 start_char, end_char를 이용해 응답 토큰의 위치를 찾음\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "# 전처리 함수를 데이터에 적용, 원래 squad에 있던 항목들은 제거\n",
    "tokenized_squad = squad.map(preprocess, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "tokenized_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48741b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf90c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 66364418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.362121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.767500</td>\n",
       "      <td>1.901501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.767500</td>\n",
       "      <td>1.783696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./QandA\\checkpoint-500\n",
      "Configuration saved in ./QandA\\checkpoint-500\\config.json\n",
      "Model weights saved in ./QandA\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./QandA\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./QandA\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.354102294921875, metrics={'train_runtime': 66.2499, 'train_samples_per_second': 181.132, 'train_steps_per_second': 11.321, 'total_flos': 1175877900288000.0, 'train_loss': 2.354102294921875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./QandA\",            # 모형 예측과 체크포인트 저장 폴더, 반드시 필요\n",
    "    evaluation_strategy=\"epoch\",     # 평가 단위, 여기서는 epoch를 선택\n",
    "    learning_rate=2e-5,              # 학습률\n",
    "    per_device_train_batch_size=16,  # 학습에 사용할 배치 크기\n",
    "    per_device_eval_batch_size=16,   # 평가에 사용할 배치 크기\n",
    "    num_train_epochs=3,              # 에포크 수\n",
    "    weight_decay=0.01,               # 가중치 감쇠 값\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7e2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: tensor(71, device='cuda:0') , end: tensor(74, device='cuda:0')\n",
      "질의: The dance capital of the world is what city in the US?\n",
      "응답: new york city\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \", end:\", answer_end)\n",
    "\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0] \n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환하고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    ")\n",
    "\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer) #응답을 만들지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31d6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./QandA\n",
      "Configuration saved in ./QandA\\config.json\n",
      "Model weights saved in ./QandA\\pytorch_model.bin\n",
      "tokenizer config file saved in ./QandA\\tokenizer_config.json\n",
      "Special tokens file saved in ./QandA\\special_tokens_map.json\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./QandA\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./QandA\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./QandA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at ./QandA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./QandA\")  # 모형 저장\n",
    "# 저장된 모형 로드\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./QandA\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./QandA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139243f0",
   "metadata": {},
   "source": [
    "## 5. 한글 질의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd379b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"수원 화성은 언제 완성되었는가?\"\n",
    "context = \"\"\"수원 화성은 조선시대 화성유수부 시가지를 둘러싼 성곽이다. \n",
    "1789년(정조 13) 수원을 팔달산 동쪽 아래로 옮기고, \n",
    "1794년(정조 18) 축성을 시작해 1796년에 완성했다.\"\"\"\n",
    "context = context.strip().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad4a317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8164e49e35124062881900cdc34cd20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e258ed9746114ae8bfd9dd45b7140f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e18852654a84a27a10cfa8a6b394294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9023bb4f1e69408bbb7335c100afc83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-small-v2-distilled-korquad-384\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\config.json\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a88f2dc73745fd8eae937ef6ac626e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\70c28f5b9e6b2bd05bb609f6be1f9f8ff918cd6f\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForQuestionAnswering.\n",
      "\n",
      "All the weights of ElectraForQuestionAnswering were initialized from the model checkpoint at monologg/koelectra-small-v2-distilled-korquad-384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9962994456291199, 'start': 87, 'end': 93, 'answer': '1796년에'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "question_answerer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "answer = question_answerer({\n",
    "    \"question\": question,\n",
    "    \"context\": context,\n",
    "})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "368148d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: tensor(57) , end: tensor(60)\n",
      "질의: 수원 화성은 언제 완성되었는가?\n",
      "응답: 1796년\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \", end:\", answer_end)\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0] \n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환하고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0df0a63f4e0adb66fb304c0d09bd41684af67dc7d74ed355fbec2150f26599e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
