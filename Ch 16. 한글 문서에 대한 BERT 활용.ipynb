{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9ca054",
   "metadata": {},
   "source": [
    "# Ch 16. í•œê¸€ë¬¸ì„œì— ëŒ€í•œ BERT í™œìš©\n",
    "\n",
    "\n",
    "## 16.1 ë‹¤ì¤‘ ì–¸ì–´ë¥¼ ìœ„í•œ(multilingual) BERT ì‚¬ì „í•™ìŠµëª¨í˜•ì˜ ë¯¸ì„¸ì¡°ì •í•™ìŠµì„ ì´ìš©í•œ í•œê¸€ ë¬¸ì„œì˜ ë¶„ë¥˜\n",
    "\n",
    "\n",
    "(1) https://github.com/google-research/bert/blob/master/multilingual.md\n",
    "<br>(2) https://huggingface.co/transformers/pretrained_models.html\n",
    "<br>(3) https://github.com/SKTBrain/KoBERT\n",
    "<br>(4) https://github.com/Beomi/KcBERT\n",
    "<br>(5) https://github.com/Beomi/KcELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaf66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# ratingì´ 6ë³´ë‹¤ ì‘ìœ¼ë©´ 0 ì¦‰ ë¶€ì •, 6 ì´ìƒì´ë©´ ê¸ì •ìœ¼ë¡œ ë¼ë²¨ ìƒì„±\n",
    "y = [0 if rate < 6 else 1 for rate in df.rating]\n",
    "# ë°ì´í„°ì…‹ì„ í•™ìŠµ, ê²€ì¦, í‰ê°€ì˜ ì„¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160ac054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"accuracy\")\n",
    "# metric ê´€ë ¨ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ datasetsì—ì„œ evaluateë¡œ ë¶„ë¦¬ë˜ë©´ì„œ ì´ì „ ì½”ë“œëŠ” ì‘ë™í•˜ì§€ ì•Šê²Œ ë¨\n",
    "# evalute íŒ¨í‚¤ì§€ ì°¸ê³ : https://huggingface.co/docs/evaluate/a_quick_tour\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5042229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378ae4fed17b4a9e928b1ba91f96e525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\momdad\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb19724700049cfad7e31cf25d92fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46737d2ee1b1440980b427804a566add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b7a4b205c64181ba8974a4da20a0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•ˆ', '##ë…•', '##í•˜', '##ì„¸', '##ìš”', '.', 'ë°˜', '##ê°‘', '##ìŠµ', '##ë‹ˆë‹¤', '.']\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\"))\n",
    "inputs = tokenizer(\"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e8780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696fa65f8442439d84054287820c51ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='518' max='518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [518/518 07:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.331410</td>\n",
       "      <td>0.863455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=518, training_loss=0.405373900093167, metrics={'train_runtime': 475.1996, 'train_samples_per_second': 34.857, 'train_steps_per_second': 1.09, 'total_flos': 2689808985606720.0, 'train_loss': 0.405373900093167, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# í† í°í™”\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# bert-base-multilingual-cased ì‚¬ì „í•™ìŠµëª¨í˜•ìœ¼ë¡œë¶€í„° ë¶„ë¥˜ê¸° ëª¨í˜•ì„ ìƒì„±\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Trainerì—ì„œ ì‚¬ìš©í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì§€ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # ëª¨í˜• ì˜ˆì¸¡ì´ë‚˜ ì²´í¬í¬ì¸íŠ¸ ì¶œë ¥ í´ë”, ë°˜ë“œì‹œ í•„ìš”í•¨\n",
    "    num_train_epochs=2,              # í•™ìŠµ ì—í¬í¬ ìˆ˜\n",
    "    evaluation_strategy=\"steps\",      # epochë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì¶œë ¥\n",
    "    eval_steps = 500,                # \n",
    "    per_device_train_batch_size=8,   # í•™ìŠµì— ì‚¬ìš©í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    per_device_eval_batch_size=16,   # í‰ê°€ì— ì‚¬ìš©í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "# Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,                     # í•™ìŠµí•  ëª¨í˜•\n",
    "    args=training_args,              # ìœ„ì—ì„œ ì •ì˜í•œ í•™ìŠµ ë§¤ê°œë³€ìˆ˜\n",
    "    train_dataset=train_dataset,     # í›ˆë ¨ ë°ì´í„°ì…‹\n",
    "    eval_dataset=val_dataset,        # ê²€ì¦ ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ë¯¸ì„¸ì¡°ì •í•™ìŠµ ì‹¤í–‰\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaf4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b83935",
   "metadata": {},
   "source": [
    "ì•„ë˜ì™€ ê°™ì´ ë¯¸ì„¸ì¡°ì •í•™ìŠµì„ ë§ˆì¹œ ëª¨í˜•ìœ¼ë¡œ í‰ê°€ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì„±ëŠ¥ì„ ì¸¡ì •í•´ë³¸ë‹¤. 80.1% ì •ë„ë¡œ ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5512b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35008788108825684,\n",
       " 'eval_accuracy': 0.8432916892992939,\n",
       " 'eval_runtime': 24.281,\n",
       " 'eval_samples_per_second': 151.641,\n",
       " 'eval_steps_per_second': 2.389,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528db2",
   "metadata": {},
   "source": [
    "## 16.2 KoBERT ì‚¬ì „í•™ìŠµëª¨í˜•ì— ëŒ€í•œ íŒŒì´í† ì¹˜ ê¸°ë°˜ ë¯¸ì„¸ì¡°ì •í•™ìŠµ\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4c0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install \"git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c9af5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eab6a290a2145ff914b55806c1c3744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\momdad\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44015ba8a2da4123bc12e19292867a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/371k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64a2465126e49dcbc7e7197bfc94bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–ì•ˆ', 'ë…•', 'í•˜ì„¸ìš”', '.', 'â–ë°˜', 'ê°‘', 'ìŠµë‹ˆë‹¤', '.']\n",
      "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5345, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\"))\n",
    "inputs = tokenizer(\"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82152a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c6e7e18bed4fe790a57985c5cd7f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34ab05f4e7142e9bb817bc95a46c51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# í† í°í™”\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ìƒì„±\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# KoBERT ì‚¬ì „í•™ìŠµëª¨í˜• ë¡œë“œ\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# BERTë¥¼ í¬í•¨í•œ ì‹ ê²½ë§ ëª¨í˜•\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels): \n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        # ë¶„ë¥˜ê¸° ì •ì˜\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # BERT ëª¨í˜•ì— ì…ë ¥ì„ ë„£ê³  ì¶œë ¥ì„ ë°›ìŒ\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT ì¶œë ¥ì—ì„œ CLS í† í°ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "# token_sizeëŠ” BERT í† í°ê³¼ ë™ì¼, bert_model.config.hidden_sizeë¡œ ì•Œ ìˆ˜ ìˆìŒ\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94646422",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee83e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_8368\\543508717.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, elapsed time: 90.12, train loss: 0.5347, validation loss: 0.4748\n",
      "Step 1000, elapsed time: 180.87, train loss: 0.5649, validation loss: 0.5470\n",
      "Step 1500, elapsed time: 271.69, train loss: 0.5521, validation loss: 0.4981\n",
      "Step 2000, elapsed time: 362.87, train loss: 0.5393, validation loss: 0.4950\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# GPU ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´ deviceë¥¼ cudaë¡œ ì„¤ì •í•˜ê³ , ì•„ë‹ˆë©´ cpuë¡œ ì„¤ì •\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)  # ëª¨í˜•ì„ GPUë¡œ ë³µì‚¬\n",
    "model.train()     # í•™ìŠµëª¨ë“œë¡œ ì „í™˜\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì œê³µí•˜ëŠ” AdamWë¡œ ì„¤ì •\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # ê°€ì¤‘ì¹˜ ê°ì‡  ì„¤ì •\n",
    "criterion = torch.nn.CrossEntropyLoss()    # ë©€í‹°í´ë˜ìŠ¤ì´ë¯€ë¡œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ë¥¼ ì†ì‹¤í•¨ìˆ˜ë¡œ ì‚¬ìš©\n",
    "\n",
    "num_epochs = 2      # í•™ìŠµ epochë¥¼ 3íšŒë¡œ ì„¤ì •\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "# í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "start = time.time() # ì‹œì‘ì‹œê°„ ê¸°ë¡\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #total_epoch_loss = 0  # epochì˜ ì´ loss ì´ˆê¸°í™”\n",
    "    for batch in train_loader:\n",
    "        model.train()     # í•™ìŠµëª¨ë“œë¡œ ì „í™˜\n",
    "        optim.zero_grad()     # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        # ë°°ì¹˜ì—ì„œ labelì„ ì œì™¸í•œ ì…ë ¥ë§Œ ì¶”ì¶œí•˜ì—¬  GPUë¡œ ë³µì‚¬\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n",
    "        labels = batch['labels'].to(device) # ë°°ì¹˜ì—ì„œ ë¼ë²¨ì„ ì¶”ì¶œí•˜ì—¬ GPUë¡œ ë³µì‚¬\n",
    "        outputs = model(inputs) # ëª¨í˜•ìœ¼ë¡œ ê²°ê³¼ ì˜ˆì¸¡\n",
    "        # ë‘ í´ë˜ìŠ¤ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê³  ê°ê° ë¹„êµí•´ì•¼ í•˜ë¯€ë¡œ labelsì— ëŒ€í•´ ì›í•«ì¸ì½”ë”©ì„ ì ìš©í•œ í›„ì— ì†ì‹¤ì„ ê²Œì‚°\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss ê³„ì‚°\n",
    "\n",
    "        train_loss += loss\n",
    "        loss.backward() # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "        optim.step()    # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "        scheduler.step() # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "        \n",
    "        step += 1\n",
    "        if step % eval_steps == 0: # eval_steps ë§ˆë‹¤ ê²½ê³¼í•œ ì‹œê°„ê³¼ lossë¥¼ ì¶œë ¥\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss ê³„ì‚°\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_train_loss = train_loss / eval_steps\n",
    "            elapsed = time.time() - start\n",
    "            print('Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' \n",
    "                  % (step, elapsed, avg_train_loss, avg_val_loss))\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3df14ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_8368\\543508717.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7645301466594242}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"accuracy\")\n",
    "# metric ê´€ë ¨ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ datasetsì—ì„œ evaluateë¡œ ë¶„ë¦¬ë˜ë©´ì„œ ì´ì „ ì½”ë“œëŠ” ì‘ë™í•˜ì§€ ì•Šê²Œ ë¨\n",
    "# evalute íŒ¨í‚¤ì§€ ì°¸ê³ : https://huggingface.co/docs/evaluate/a_quick_tour\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad(): # í•™ìŠµí•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë”\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f60783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
