{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9ca054",
   "metadata": {},
   "source": [
    "# Ch 16. 한글문서에 대한 BERT 활용\n",
    "\n",
    "\n",
    "## 16.1 다중 언어를 위한(multilingual) BERT 사전학습모형의 미세조정학습을 이용한 한글 문서의 분류\n",
    "\n",
    "\n",
    "(1) https://github.com/google-research/bert/blob/master/multilingual.md\n",
    "<br>(2) https://huggingface.co/transformers/pretrained_models.html\n",
    "<br>(3) https://github.com/SKTBrain/KoBERT\n",
    "<br>(4) https://github.com/Beomi/KcBERT\n",
    "<br>(5) https://github.com/Beomi/KcELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaf66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# rating이 6보다 작으면 0 즉 부정, 6 이상이면 긍정으로 라벨 생성\n",
    "y = [0 if rate < 6 else 1 for rate in df.rating]\n",
    "# 데이터셋을 학습, 검증, 평가의 세 데이터셋으로 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160ac054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"accuracy\")\n",
    "# metric 관련된 라이브러리가 datasets에서 evaluate로 분리되면서 이전 코드는 작동하지 않게 됨\n",
    "# evalute 패키지 참고: https://huggingface.co/docs/evaluate/a_quick_tour\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5042229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378ae4fed17b4a9e928b1ba91f96e525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\momdad\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb19724700049cfad7e31cf25d92fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46737d2ee1b1440980b427804a566add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b7a4b205c64181ba8974a4da20a0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e8780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696fa65f8442439d84054287820c51ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='518' max='518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [518/518 07:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.331410</td>\n",
       "      <td>0.863455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=518, training_loss=0.405373900093167, metrics={'train_runtime': 475.1996, 'train_samples_per_second': 34.857, 'train_steps_per_second': 1.09, 'total_flos': 2689808985606720.0, 'train_loss': 0.405373900093167, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# bert-base-multilingual-cased 사전학습모형으로부터 분류기 모형을 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Trainer에서 사용할 하이퍼 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n",
    "    num_train_epochs=2,              # 학습 에포크 수\n",
    "    evaluation_strategy=\"steps\",      # epoch마다 검증 데이터셋에 대한 평가 지표를 출력\n",
    "    eval_steps = 500,                # \n",
    "    per_device_train_batch_size=8,   # 학습에 사용할 배치 사이즈\n",
    "    per_device_eval_batch_size=16,   # 평가에 사용할 배치 사이즈\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 학습할 모형\n",
    "    args=training_args,              # 위에서 정의한 학습 매개변수\n",
    "    train_dataset=train_dataset,     # 훈련 데이터셋\n",
    "    eval_dataset=val_dataset,        # 검증 데이터셋\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 미세조정학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaf4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b83935",
   "metadata": {},
   "source": [
    "아래와 같이 미세조정학습을 마친 모형으로 평가 데이터셋에 대해 성능을 측정해본다. 80.1% 정도로 나쁘지 않은 성능을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5512b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_14876\\3889917264.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35008788108825684,\n",
       " 'eval_accuracy': 0.8432916892992939,\n",
       " 'eval_runtime': 24.281,\n",
       " 'eval_samples_per_second': 151.641,\n",
       " 'eval_steps_per_second': 2.389,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528db2",
   "metadata": {},
   "source": [
    "## 16.2 KoBERT 사전학습모형에 대한 파이토치 기반 미세조정학습\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4c0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install \"git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c9af5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eab6a290a2145ff914b55806c1c3744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\momdad\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44015ba8a2da4123bc12e19292867a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/371k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64a2465126e49dcbc7e7197bfc94bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '.', '▁반', '갑', '습니다', '.']\n",
      "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5345, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82152a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c6e7e18bed4fe790a57985c5cd7f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34ab05f4e7142e9bb817bc95a46c51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# KoBERT 사전학습모형 로드\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# BERT를 포함한 신경망 모형\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels): \n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        # 분류기 정의\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # BERT 모형에 입력을 넣고 출력을 받음\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "# token_size는 BERT 토큰과 동일, bert_model.config.hidden_size로 알 수 있음\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94646422",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee83e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_8368\\543508717.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "C:\\Users\\momdad\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, elapsed time: 90.12, train loss: 0.5347, validation loss: 0.4748\n",
      "Step 1000, elapsed time: 180.87, train loss: 0.5649, validation loss: 0.5470\n",
      "Step 1500, elapsed time: 271.69, train loss: 0.5521, validation loss: 0.4981\n",
      "Step 2000, elapsed time: 362.87, train loss: 0.5393, validation loss: 0.4950\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)  # 모형을 GPU로 복사\n",
    "model.train()     # 학습모드로 전환\n",
    "\n",
    "# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()    # 멀티클래스이므로 크로스 엔트로피를 손실함수로 사용\n",
    "\n",
    "num_epochs = 2      # 학습 epoch를 3회로 설정\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "# 학습 스케줄러 설정\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "start = time.time() # 시작시간 기록\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #total_epoch_loss = 0  # epoch의 총 loss 초기화\n",
    "    for batch in train_loader:\n",
    "        model.train()     # 학습모드로 전환\n",
    "        optim.zero_grad()     # 그래디언트 초기화\n",
    "        # 배치에서 label을 제외한 입력만 추출하여  GPU로 복사\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n",
    "        labels = batch['labels'].to(device) # 배치에서 라벨을 추출하여 GPU로 복사\n",
    "        outputs = model(inputs) # 모형으로 결과 예측\n",
    "        # 두 클래스에 대해 예측하고 각각 비교해야 하므로 labels에 대해 원핫인코딩을 적용한 후에 손실을 게산\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "\n",
    "        train_loss += loss\n",
    "        loss.backward() # 그래디언트 계산\n",
    "        optim.step()    # 가중치 업데이트\n",
    "        scheduler.step() # 스케줄러 업데이트\n",
    "        \n",
    "        step += 1\n",
    "        if step % eval_steps == 0: # eval_steps 마다 경과한 시간과 loss를 출력\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_train_loss = train_loss / eval_steps\n",
    "            elapsed = time.time() - start\n",
    "            print('Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' \n",
    "                  % (step, elapsed, avg_train_loss, avg_val_loss))\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3df14ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momdad\\AppData\\Local\\Temp\\ipykernel_8368\\543508717.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7645301466594242}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"accuracy\")\n",
    "# metric 관련된 라이브러리가 datasets에서 evaluate로 분리되면서 이전 코드는 작동하지 않게 됨\n",
    "# evalute 패키지 참고: https://huggingface.co/docs/evaluate/a_quick_tour\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad(): # 학습할 필요가 없으므로 그래디언트 계산을 끔\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f60783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
